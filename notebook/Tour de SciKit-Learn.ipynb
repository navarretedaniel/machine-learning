{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tour of SciKit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about Data Science and the Data Science Pipeline, we are typically talking about the management of data flows for a specific purpose - the modeling of some hypothesis. The models that we construct can then be used in Data Products as an engine to create more data and actionable results. Machine learning is the art of training some model by using existing data along with a statistical method to create a parametric representation of a model that fits the data. That’s kind of a mouthful, but what that essentially means is that a machine learning algorithm uses statistical processes to learn from examples, then applies what it has learned to future inputs to predict an outcome.\n",
    "\n",
    "Machine learning can classically be summarized with two methodologies: supervised and unsupervised learning. In supervised learning, the “correct answers” are annotated ahead of time and the algorithm tries to fit a decision space based on those answers. In unsupervised learning, algorithms try to group like examples together, inferring similarities via distance metrics. Machine learning allows us to handle new data in a meaningful way, predicting where new data will fit into our models. \n",
    "\n",
    "Scikit-Learn is a powerful machine learning library implemented in Python with numeric and scientific computing powerhouses Numpy, Scipy, and matplotlib for extremely fast analysis of small to medium sized data sets. It is open source, commercially usable and contains many modern machine learning algorithms for classification, regression, clustering, feature extraction, and optimization. For this reason Scikit-Learn is often the first tool in a Data Scientists toolkit for machine learning of incoming data sets. \n",
    "\n",
    "The purpose of this notebook is to serve as an introduction to Machine Learning with Scikit-Learn. We will explore several clustering, classification, and regression algorithms. In particular, we will structure our machine learning models as though we were producing a data product, an actionable model that can be used in larger programs or algorithms; rather than as simply a research or investigation methodology. For more on Scikit-Learn see: [Six Reasons why I recommend Scikit-Learn (O’Reilly Radar)](http://radar.oreilly.com/2013/12/six-reasons-why-i-recommend-scikit-learn.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Things we'll need later\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the example datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_linnerud\n",
    "\n",
    "# Load warnings to filter out future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Boston house prices dataset (reals, regression)\n",
    "boston = load_boston()\n",
    "print(\"Boston: %i samples %i features\" % boston.data.shape)\n",
    "\n",
    "# Iris flower dataset (reals, multi-label classification)\n",
    "iris   = load_iris()\n",
    "print(\"Iris: %i samples %i features\" % iris.data.shape)\n",
    "\n",
    "# Diabetes dataset (reals, regression)\n",
    "diabetes = load_diabetes()\n",
    "print(\"Diabetes: %i samples %i features\" % diabetes.data.shape)\n",
    "\n",
    "# Hand-written digit dataset (multi-label classification)\n",
    "digits = load_digits()\n",
    "print(\"Digits: %i samples %i features\" % digits.data.shape)\n",
    "\n",
    "# Linnerud psychological and exercise dataset (multivariate regression)\n",
    "linnerud = load_linnerud()\n",
    "print(\"Linnerud: %i samples %i features\" % linnerud.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets that come with Scikit Learn demonstrate the properties of classification and regression algorithms, as well as how the data should fit. They are also small and are easy to train models that work. As such they are ideal for pedagogical purposes. The `datasets` module also contains functions for loading data from the [mldata.org](http://mldata.org) repository as well as for generating random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix, radviz\n",
    "\n",
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = iris.feature_names\n",
    "\n",
    "cmap = {0:'r', 1:'b', 2:'g'}\n",
    "colors = [cmap[t] for t in iris.target]\n",
    "fig = scatter_matrix(df, alpha=0.4, figsize=(16, 10), diagonal='kde', color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "df['species'] = [iris.target_names[idx] for idx in iris.target]\n",
    "radviz(df,'species', color=['r', 'g', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(diabetes.data)\n",
    "fig = scatter_matrix(df, alpha=0.2, figsize=(16, 10), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressions are a type of supervised learning algorithm, where, given continuous input data, the object is to fit a function that is able to predict the continuous value of input features.\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression fits a linear model (a line in two dimensions) to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit regression to diabetes dataset\n",
    "model = LinearRegression()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "A primitive neural network that learns weights for input vectors and transfers the weights through a network to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "model = Perceptron()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor Regression\n",
    "\n",
    "Makes predictions by locating similar cases and returning the average majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees (CART)\n",
    "\n",
    "Makes splits of the best separation of the data for the predictions being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forest is an ensemble method that creates a number of decision trees using the CART algorithm, each on a different subset of the data. The general approach to creating the ensemble is bootstrap aggregation of the decision trees (bagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "Adaptive Boosting (AdaBoost) is an ensemble method that sums the predictions made by multiple decision trees. Additional models are added and trained on instances that were incorrectly predicted (boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Uses the SVM algorithm (transforming the problem space into higher dimensions in order to use kernel methods) to make predictions for a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR()\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization methods decrease the over-fitting of a model by penalizing complexity. These are usually demonstrated on regression algorithms, which is why they are included in this section.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Also known as Tikhonov regularization penalizes a least squares regression model on the square of the absolute magnitiude of the coefficients (the L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=0.1)\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (LASSO) penalizes the least squares regression on the absolute magnitude of the coefficients (the L1 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(diabetes.data, diabetes.target)\n",
    "\n",
    "expected  = diabetes.target\n",
    "predicted = model.predict(diabetes.data)\n",
    "\n",
    "# Evaluate fit of the model\n",
    "print(\"Mean Squared Error: %0.3f\" % mse(expected, predicted))\n",
    "print(\"Coefficient of Determination: %0.3f\" % r2_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a supervised machine learning problem where, given labeled input data (with two or more labels), the task is to fit a function that can predict the discrete class of input data.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Fits a logistic model to data and makes predictions about the probability of a categorical event (between 0 and 1). Logistic regressions make predictions between 0 and 1, so in order to classify multiple classes a one-vs-all scheme is used (one model per class, winner-takes-all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "splits     = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model      = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected   = y_test\n",
    "predicted  = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "Linear Discriminate Analysis (LDA) fits a conditional probability density function (Gaussian) to the attributes of the classes. The discrimination function is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "splits     = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model      = LDA()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected   = y_test\n",
    "predicted  = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Uses Bayes Theorem (with a naive assumption) to model the conditional relationship of each attribute to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "splits     = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model      = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected   = y_test\n",
    "predicted  = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor\n",
    "\n",
    "Makes predictions by locating similar instances via a similarity function or distance and averaging the majority of the most similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "splits = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model= KNeighborsClassifier(20)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))\n",
    "\n",
    "#help(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees use the CART algorithm to make predictions by making splits that best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "splits = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))\n",
    "\n",
    "#help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs\n",
    "\n",
    "Support Vector Machines (SVM) uses points in transformed problem space that separates the classes into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "\n",
    "splits     = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "for kernel in kernels:\n",
    "    if kernel != 'poly':\n",
    "        model      = SVC(kernel=kernel)\n",
    "    else:\n",
    "        model      = SVC(kernel=kernel, degree=3)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    expected   = y_test\n",
    "    predicted  = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is an ensemble of decision trees on different subsets of the dataset. The ensemble is created by bootstrap aggregation (bagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "splits     = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "model      = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "expected   = y_test\n",
    "predicted  = model.predict(X_test)\n",
    "\n",
    "print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms attempt to find patterns in unlabeled data. They are usually grouped into two main categories: centroidal (find the centers of clusters) and hierarchical (find clusters of clusters).\n",
    "\n",
    "In order to explore clustering, we'll have to generate some fake datasets to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N = 1000 # Number of samples in each cluster\n",
    "\n",
    "# Some colors for later\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "circles = make_circles(n_samples=N, factor=.5, noise=.05)\n",
    "moons   = make_moons(n_samples=N, noise=.08)\n",
    "blobs   = make_blobs(n_samples=N, random_state=9)\n",
    "noise   = np.random.rand(N, 2), None\n",
    "\n",
    "# Let's see what the data looks like!\n",
    "fig, axe = plt.subplots(figsize=(18, 4))\n",
    "for idx, dataset in enumerate((circles, moons, blobs, noise)):\n",
    "    X, y = dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.scatter(X[:,0], X[:,1], marker='.')\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.xlabel('$x_0$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "Partition N samples into k clusters, where each sample belongs to a cluster to which it has the closest mean of the neighbors. This problem is NP-hard, but there are good estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(18, 4))\n",
    "for idx, dataset in enumerate((circles, moons, blobs, noise)):\n",
    "    X, y = dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Fit the model with our algorithm\n",
    "    model = MiniBatchKMeans(n_clusters=5)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make Predictions\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Find centers\n",
    "    centers = model.cluster_centers_\n",
    "    center_colors = colors[:len(centers)]\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "    \n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], color=colors[predictions].tolist(), s=10)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.xlabel('$x_0$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation\n",
    "\n",
    "Clustering based on the concept of \"message passing\" between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not require the number of clusters to be determined or estimated before running the algorithm. Like k-medoids, AP finds \"exemplars\", members of the input set that are representative of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(18, 4))\n",
    "for idx, dataset in enumerate((circles, moons, blobs, noise)):\n",
    "    X, y = dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Fit the model with our algorithm\n",
    "    model = AffinityPropagation(damping=.92, preference=-200)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make Predictions\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Find centers\n",
    "    centers = model.cluster_centers_\n",
    "    center_colors = colors[:len(centers)]\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n",
    "    \n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], color=colors[predictions].tolist(), s=10)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.ylabel('$x_1$')\n",
    "    plt.xlabel('$x_0$')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
